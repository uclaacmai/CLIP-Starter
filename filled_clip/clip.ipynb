{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP (Contrastive Language-Image Pre-Training)\n",
    "\n",
    "In this notebook, we will be exploring OpenAI's [CLIP](https://openai.com/research/clip) model. CLIP is a deep learning model that learns to *associate images and text*. It is trained on a variety of image-text pairs, and learns to predict which image goes with which text. This allows it to perform a variety of tasks, such as zero-shot image classification, image generation, and text-to-image generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tqdm.auto as tqdm\n",
    "\n",
    "# this automatically reloads the libraries so you can update them dynamically\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIP in Action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect the required packages and the CLIP model.\n",
    "*For people runing this notebook locally*: You can use a fresh conda environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ftfy\n",
      "  Obtaining dependency information for ftfy from https://files.pythonhosted.org/packages/f4/f0/21efef51304172736b823689aaf82f33dbc64f54e9b046b75f5212d5cee7/ftfy-6.2.0-py3-none-any.whl.metadata\n",
      "  Downloading ftfy-6.2.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting regex\n",
      "  Obtaining dependency information for regex from https://files.pythonhosted.org/packages/a8/01/18232f93672c1d530834e2e0568a80eaab1df12d67ae499b1762ab462b5c/regex-2023.12.25-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading regex-2023.12.25-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/42.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/42.0 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/42.0 kB ? eta -:--:--\n",
      "     ------------------ ------------------- 20.5/42.0 kB 222.6 kB/s eta 0:00:01\n",
      "     --------------------------- ---------- 30.7/42.0 kB 262.6 kB/s eta 0:00:01\n",
      "     -------------------------------------- 42.0/42.0 kB 253.9 kB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (4.65.0)\n",
      "Collecting wcwidth<0.3.0,>=0.2.12 (from ftfy)\n",
      "  Obtaining dependency information for wcwidth<0.3.0,>=0.2.12 from https://files.pythonhosted.org/packages/fd/84/fd2ba7aafacbad3c4201d395674fc6348826569da3c0937e75505ead3528/wcwidth-0.2.13-py2.py3-none-any.whl.metadata\n",
      "  Downloading wcwidth-0.2.13-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: colorama in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Downloading ftfy-6.2.0-py3-none-any.whl (54 kB)\n",
      "   ---------------------------------------- 0.0/54.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 54.4/54.4 kB 1.4 MB/s eta 0:00:00\n",
      "Downloading regex-2023.12.25-cp311-cp311-win_amd64.whl (269 kB)\n",
      "   ---------------------------------------- 0.0/269.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 269.5/269.5 kB 8.4 MB/s eta 0:00:00\n",
      "Downloading wcwidth-0.2.13-py2.py3-none-any.whl (34 kB)\n",
      "Installing collected packages: wcwidth, regex, ftfy\n",
      "  Attempting uninstall: wcwidth\n",
      "    Found existing installation: wcwidth 0.2.6\n",
      "    Uninstalling wcwidth-0.2.6:\n",
      "      Successfully uninstalled wcwidth-0.2.6\n",
      "Successfully installed ftfy-6.2.0 regex-2023.12.25 wcwidth-0.2.13\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to c:\\users\\johnj\\appdata\\local\\temp\\pip-req-build-qc5zgx7d\n",
      "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: ftfy in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from clip==1.0) (6.2.0)\n",
      "Requirement already satisfied: regex in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from clip==1.0) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from clip==1.0) (4.65.0)\n",
      "Requirement already satisfied: torch in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from clip==1.0) (2.2.0)\n",
      "Requirement already satisfied: torchvision in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from clip==1.0) (0.17.0)\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from ftfy->clip==1.0) (0.2.13)\n",
      "Requirement already satisfied: filelock in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from torch->clip==1.0) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from torch->clip==1.0) (4.9.0)\n",
      "Requirement already satisfied: sympy in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from torch->clip==1.0) (1.12)\n",
      "Requirement already satisfied: networkx in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from torch->clip==1.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from torch->clip==1.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from torch->clip==1.0) (2024.2.0)\n",
      "Requirement already satisfied: numpy in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from torchvision->clip==1.0) (1.25.1)\n",
      "Requirement already satisfied: requests in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from torchvision->clip==1.0) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from torchvision->clip==1.0) (9.4.0)\n",
      "Requirement already satisfied: colorama in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from tqdm->clip==1.0) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from jinja2->torch->clip==1.0) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from requests->torchvision->clip==1.0) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from requests->torchvision->clip==1.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from requests->torchvision->clip==1.0) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from requests->torchvision->clip==1.0) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\anaconda3\\envs\\pytorch\\lib\\site-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
      "Building wheels for collected packages: clip\n",
      "  Building wheel for clip (setup.py): started\n",
      "  Building wheel for clip (setup.py): finished with status 'done'\n",
      "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369578 sha256=9399f8e40d4778119d784d2aa3cd97d4c0de814fa9a8aedac4433eb7e7bc4a7d\n",
      "  Stored in directory: C:\\Users\\johnj\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-q1bvfp7t\\wheels\\3f\\7c\\a4\\9b490845988bf7a4db33674d52f709f088f64392063872eb9a\n",
      "Successfully built clip\n",
      "Installing collected packages: clip\n",
      "Successfully installed clip-1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git 'C:\\Users\\johnj\\AppData\\Local\\Temp\\pip-req-build-qc5zgx7d'\n"
     ]
    }
   ],
   "source": [
    "# uncomment the line below if you are not in colab\n",
    "#%conda install --yes -c pytorch pytorch=1.7.1 torchvision cudatoolkit=11.0\n",
    "%pip install ftfy regex tqdm\n",
    "%pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be working with the following diagram of the CLIP architecture:\n",
    "<img src='https://github.com/openai/CLIP/blob/main/CLIP.png?raw=true'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# download the clip diagram\n",
    "#!wget https://github.com/openai/CLIP/blob/main/CLIP.png?raw=true -O CLIP.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's load a pretrained CLIP model with a base (B) size Vision Transformer (ViT) that uses 32 patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 338M/338M [05:43<00:00, 1.03MiB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\functional.py:5476: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label probs: [[0.9927   0.004185 0.002968]]\n"
     ]
    }
   ],
   "source": [
    "# TODO: try your own images and text labels\n",
    "im = \"CLIP.png\" \n",
    "image = preprocess(Image.open(im)).unsqueeze(0).to(device)\n",
    "text = clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"]).to(device) \n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "    \n",
    "    logits_per_image, logits_per_text = model(image, text)\n",
    "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "print(\"Label probs:\", probs)  # prints: [[0.9927937  0.00421068 0.00299572]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
